{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Carregamento e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ks-projects-201801.csv')\n",
    "\n",
    "def to_time(str, mask):\n",
    "    return dt.strptime(str, mask)\n",
    "\n",
    "def col_dict(dataframe, col):\n",
    "    unique_values = dataframe[col].unique()\n",
    "    dc = {}\n",
    "\n",
    "    for idx,val in enumerate(unique_values):\n",
    "        dc[val] = idx\n",
    "\n",
    "    return dc\n",
    "\n",
    "def to_numeric(dataframe, col):\n",
    "    \"\"\" Transforms the values of column col to a numeric mapping.\n",
    "        Returns the transformed dataframe and the dictionary with the mapping.\"\"\"\n",
    "    df = dataframe\n",
    "    dc = col_dict(df, col)\n",
    "    df[col] = df[col].apply(lambda x: dc[x])\n",
    "\n",
    "    return df, dc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "State feature transformation:\n",
    "\n",
    "1 - Removing projects with state equals to 'undefined' or 'live'\n",
    "2 - Change the values to make a binary classification:\n",
    "\n",
    "successful: 1\n",
    "failed: 0\n",
    "canceled: 0\n",
    "suspended': 0\n",
    "\"\"\"\n",
    "\n",
    "df.drop(df[(df.state == 'live') | (df.state == 'undefined')].index, inplace=True)\n",
    "df['state'] = (df['state'] == 'successful').astype(int)\n",
    "\n",
    "\n",
    "\"\"\"Cleans and add columns from columns already in the data\"\"\"\n",
    "# Convert string to datetime and get the \n",
    "# difference in days from beginning to end of the campaign\n",
    "\n",
    "df['running_days'] = (\n",
    "    df['deadline'].apply(to_time, args=('%Y-%m-%d',)) \n",
    "    - df['launched'].apply(to_time, args=('%Y-%m-%d %H:%M:%S',))\n",
    ").apply(lambda x: x.days)\n",
    "  \n",
    "df, cat_dict = to_numeric(df, 'category')\n",
    "df, main_cat_dict = to_numeric(df, 'main_category')\n",
    "df, country_dict = to_numeric(df, 'country')\n",
    "\n",
    "# Removing unused features\n",
    "df.drop('ID', axis=1, inplace=True)\n",
    "df.drop('name', axis=1, inplace=True)\n",
    "df.drop('deadline', axis=1, inplace=True)\n",
    "df.drop('launched', axis=1, inplace=True)\n",
    "df.drop('pledged', axis=1, inplace=True)\n",
    "df.drop('usd_pledged', axis=1, inplace=True)\n",
    "df.drop('goal', axis=1, inplace=True)\n",
    "df.drop('currency', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização dos dados (média e feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_column = 'state'\n",
    "columns_to_normalize = ['running_days', 'backers', 'usd_pledged_real', 'usd_goal_real']\n",
    "cols_labels = df.columns.values\n",
    "sidx = np.argsort(cols_labels)\n",
    "index_columns = sidx[np.searchsorted(cols_labels,columns_to_normalize,sorter=sidx)]\n",
    "y_index_columns = sidx[np.searchsorted(cols_labels,y_column,sorter=sidx)]\n",
    "\n",
    "values = df.values\n",
    "values_shape = values.shape\n",
    "    \n",
    "for j in index_columns:\n",
    "    max_value = np.max(values[:,j])\n",
    "    min_value = np.min(values[:,j])\n",
    "    mean = np.sum(values[:,j]) / values_shape[0]\n",
    "\n",
    "    i = 0\n",
    "    while i < values_shape[0]:\n",
    "        values[i, j] = (values[i, j] - mean) / (max_value - min_value)\n",
    "        i = i + 1\n",
    "\n",
    "y = values[:,y_index_columns]\n",
    "X = np.concatenate((values[:, 0:y_index_columns], values[:,y_index_columns+1:values_shape[1]]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural Artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando convergência do Perceptron para verificar se os dados são linearmente separáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erickbs7/workspaces/machinelearning/env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(random_state = 0)\n",
    "perceptron.fit(X, y)\n",
    "predicted = perceptron.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN = 195659\n",
      "FP = 0\n",
      "FN = 38411\n",
      "TP = 0\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y, predicted)\n",
    "\n",
    "#plt.clf() \n",
    "#plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "# classNames = ['Negative','Positive']\n",
    "# plt.title('Perceptron Confusion Matrix - Entire Data')\n",
    "# plt.ylabel('True label')\n",
    "# plt.xlabel('Predicted label')\n",
    "# tick_marks = np.arange(len(classNames))\n",
    "# plt.xticks(tick_marks, classNames, rotation=45)\n",
    "# plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "  \n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "        #plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "\n",
    "# TODO - Apresentar o gráfico da matriz de confusão\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando convergência do Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.19129222\n",
      "Iteration 2, loss = 1.14581330\n",
      "Iteration 3, loss = 1.15306590\n",
      "Iteration 4, loss = 1.17781989\n",
      "Iteration 5, loss = 1.20699634\n",
      "Iteration 6, loss = 1.19064743\n",
      "Iteration 7, loss = 1.18469700\n",
      "Iteration 8, loss = 1.17042699\n",
      "Iteration 9, loss = 1.15402763\n",
      "Iteration 10, loss = 1.14826339\n",
      "Iteration 11, loss = 1.13899131\n",
      "Iteration 12, loss = 1.08335030\n",
      "Iteration 13, loss = 1.12904251\n",
      "Iteration 14, loss = 1.03254984\n",
      "Iteration 15, loss = 1.02353163\n",
      "Iteration 16, loss = 1.01762309\n",
      "Iteration 17, loss = 1.01760645\n",
      "Iteration 18, loss = 1.01878239\n",
      "Iteration 19, loss = 1.01654529\n",
      "Iteration 20, loss = 1.00657859\n",
      "Iteration 21, loss = 1.00522781\n",
      "Iteration 22, loss = 1.01226827\n",
      "Iteration 23, loss = 1.03912607\n",
      "Iteration 24, loss = 0.99212537\n",
      "Iteration 25, loss = 1.01019784\n",
      "Iteration 26, loss = 0.98868965\n",
      "Iteration 27, loss = 1.01085377\n",
      "Iteration 28, loss = 1.00216404\n",
      "Iteration 29, loss = 0.98443377\n",
      "Iteration 30, loss = 0.98305795\n",
      "Iteration 31, loss = 0.99104282\n",
      "Iteration 32, loss = 0.99758652\n",
      "Iteration 33, loss = 0.99903157\n",
      "Iteration 34, loss = 1.01586145\n",
      "Iteration 35, loss = 0.98055438\n",
      "Iteration 36, loss = 0.97643879\n",
      "Iteration 37, loss = 0.97582332\n",
      "Iteration 38, loss = 0.98084272\n",
      "Iteration 39, loss = 0.97420320\n",
      "Iteration 40, loss = 0.97502517\n",
      "Iteration 41, loss = 0.97046630\n",
      "Iteration 42, loss = 0.96877617\n",
      "Iteration 43, loss = 0.96938663\n",
      "Iteration 44, loss = 0.97628093\n",
      "Iteration 45, loss = 0.99822703\n",
      "Iteration 46, loss = 0.98188823\n",
      "Iteration 47, loss = 0.96997562\n",
      "Iteration 48, loss = 0.97060054\n",
      "Iteration 49, loss = 0.96282522\n",
      "Iteration 50, loss = 0.96171578\n",
      "Iteration 51, loss = 0.96074974\n",
      "Iteration 52, loss = 0.96556819\n",
      "Iteration 53, loss = 1.00062255\n",
      "Iteration 54, loss = 0.95948430\n",
      "Iteration 55, loss = 0.96152686\n",
      "Iteration 56, loss = 0.97110703\n",
      "Iteration 57, loss = 0.98772435\n",
      "Iteration 58, loss = 0.96297342\n",
      "Iteration 59, loss = 0.97031890\n",
      "Iteration 60, loss = 0.95994430\n",
      "Iteration 61, loss = 0.95991645\n",
      "Iteration 62, loss = 0.97352487\n",
      "Iteration 63, loss = 0.96766811\n",
      "Iteration 64, loss = 0.96046996\n",
      "Iteration 65, loss = 0.97771327\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "114\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "test_x = X[0:379,:]\n",
    "test_y = y[0:379]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(test_x, test_y, test_size= 0.3, random_state=27)\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(7, 50, 1), max_iter=1000, alpha=1, solver='sgd', verbose=10, random_state=21,tol=0.000000001)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, alpha=0.001, solver='sgd', verbose=10, random_state=21,tol=0.000000001)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "c = 0\n",
    "i = 0\n",
    "while i < y_test.shape[0]:\n",
    "    if y_test[i] == y_pred[i]:\n",
    "        c = c + 1\n",
    "    i = i + 1\n",
    "print(y_test.shape[0])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
